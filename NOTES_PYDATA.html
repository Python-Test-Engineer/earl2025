<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">

<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>PyData Southampton June 2025</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }

    body {
      font-family: Raleway, sans-serif;
      font-size: 20px;
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }

    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }

      h1 {
        font-size: 1.8em;
      }
    }

    @media print {
      html {
        background-color: white;
      }

      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }

      p,
      h2,
      h3 {
        orphans: 3;
        widows: 3;
      }

      h2,
      h3,
      h4 {
        page-break-after: avoid;
      }
    }

    p {
      margin: 1em 0;
    }

    a {
      color: #1a1a1a;
    }

    a:visited {
      color: #1a1a1a;
    }

    img {
      max-width: 100%;
    }

    h1,
    h2,
    h3,
    h4,
    h5,
    h6 {
      margin-top: 1.4em;
    }

    h5,
    h6 {
      font-size: 1em;
      font-style: italic;
    }

    h6 {
      font-weight: normal;
    }

    ol,
    ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }

    li>ol,
    li>ul {
      margin-top: 0;
    }

    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }

    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }

    pre {
      margin: 1em 0;
      overflow: auto;
    }

    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }

    .sourceCode {
      background-color: transparent;
      overflow: visible;
    }

    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }

    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }

    table caption {
      margin-bottom: 0.75em;
    }

    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }

    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }

    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }

    header {
      margin-bottom: 4em;
      text-align: center;
    }

    #TOC li {
      list-style: none;
    }

    #TOC ul {
      padding-left: 1.3em;
    }

    #TOC>ul {
      padding-left: 0;
    }

    #TOC a:not(:hover) {
      text-decoration: none;
    }

    code {
      white-space: pre-wrap;
    }

    span.smallcaps {
      font-variant: small-caps;
    }

    div.columns {
      display: flex;
      gap: min(4vw, 1.5em);
    }

    div.column {
      flex: auto;
      overflow-x: auto;
    }

    div.hanging-indent {
      margin-left: 1.5em;
      text-indent: -1.5em;
    }

    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class] {
      list-style: none;
    }

    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }

    .display.math {
      display: block;
      text-align: center;
      margin: 0.5rem auto;
    }
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>

<body>
  <!-- https://mconverter.eu/convert/markdown/html/ -->

  <img src="./craig-west-pydata-southampton.png" width="600px">

  <p>The repo containing all the materials is here:</p>
  <p><a href="https://pytest-cookbook.com/">https://pytest-cookbook.com/</a>
    (has link)</p>
  <p>or</p>
  <p><a
      href="https://github.com/Python-Test-Engineer/pydata-southampton">https://github.com/Python-Test-Engineer/pydata-southampton</a>
    (direct)</p>
  <p>All links used are in LINKS.md</p>
  <p>I will show you the repo and this is a mini-workshop effectively that
    can also be used for future reference.</p>
  <p>And I will be using NOTES_PYDATA.md/NOTES_PYDATA.html for this talk
    so you can follow along with me. There will repetition of some
    information between my NOTES and the NOTEBOOKS to clarify matters.</p>
  <p>The talk will be mostly code walkthroughs with notebooks that have a
    lot of comments for completeness and clarification.</p>
  <h2 id="aim">Aim</h2>
  <ol type="1">
    <li>To see that it can be just ‘AI as API’, albeit a very magical
      API.</li>
    <li>To show that AI based apps need not be all AI or not at all, but we
      can have ‘a bit of AI in our apps’.</li>
    <li>To show that it is ‘business as usual’ as Pythonistas, using our
      experience and skills to create AI Apps.</li>
  </ol>
  <p><em>I will use the term Data Engineering to cover Data Science, Data
      Analysis and other ML tasks.</em></p>
  <h3 style="color:#DB4C00;">
    Let's look at where Agentic Data Engineering differ from regular Data Engineering perhaps we may see that AI Agents
    are everyday Python with LLM API calls.
  </h3>

  <h2 id="who-am-i">Who am I?</h2>
  <p><strong>I am one of <em>US</em> - a regular Pythonista.</strong></p>
  <p>Wrestling and getting to grips with these new technologies.</p>
  <p><em>“It doesn’t get any easier - just different.” - Anon</em></p>
  <p>I was in tech in the early 2000s as a Business Information Architect
    and Certified MicroSoft SQL Server DBA. I returned in 2017 via WordPress
    and JavaScript Frameworks, moving to Python and ML in 2021.</p>
  <p>Currently, I am working on a project ‘AI Powered Knowledge Systems’,
    building a book/framework similiar to PyTest Full Stack.</p>
  <p>My links:</p>
  <ul>
    <li><a
        href="https://ai-powered-knowledge-systems.netlify.app/">https://ai-powered-knowledge-systems.netlify.app/</a>
    </li>
    <li><a href="https://pytest-cookbook.com/">https://pytest-cookbook.com/</a></li>
    <li><a href="https://django-fullstack-testing.netlify.app/">https://django-fullstack-testing.netlify.app/</a></li>
  </ul>
  <h3 id="brighton-uk">Brighton, UK</h3>
  <img src="./images/brighton-map.jpeg" height="200">
  <img src="./images/i360.jpeg" height="200">

  <h3 id="volunteer-coach">Volunteer coach</h3>
  <p>I am a volunteer coach at codebar.io/brighton</p>
  <img src="./images/codebar.png" width="400">

  <p>and I also enjoy working in community kitchens and partner dancing.
    <br><br><br><br>
  </p>
  <h3 id="leo">Leo</h3>
  <p>Just got a Red Fox Labrador Pup Leo, (much earlier than planned):</p>
  <img src="./images/leo-main.png" width="250px">

  <p>We have a local red fox that is apt to follow us…</p>
  <img src="./images/leo-fox-daylight-2.png" width="400px">

  <p>…even to our home…</p>
  <img src="./images/fox-at-door.jpg" width="400px">

  <h3 id="my-first-computer-1979">My first computer 1979</h3>
  <p>!<img src="./images/paper-tape.jpg" width="400"></p>
  <p><a
      href="https://en.wikipedia.org/wiki/Punched_tape#/media/File:Creed_model_6S-2_paper_tape_reader.jpg">https://en.wikipedia.org/wiki/Punched_tape#/media/File:Creed_model_6S-2_paper_tape_reader.jpg</a>
  </p>
  <p>…cut and paste was cut and paste!</p>
  <h1 id="demo-django-app">Demo Django App</h1>
  <p>We will look at a demo Django app that uses AI Agents to call LLMs.
    It is a chatbot style but could be a text field in a form that has
    Natural Language.</p>
  <p>I took information from the Conference website and used the code, ( a
    copy and paste), from <code>03_faq_ipynb</code> to provide a help
    chatbot…</p>
  <p>“what is a sprint and when does it take place - also I am not an
    expert coder does that matter?”</p>
  <h1 id="what-are-ai-agents">What are AI Agents?</h1>
  <p>There are many definitions:</p>
  <h2 id="anthropic">Anthropic</h2>
  <p><img src="./images/what-is-agent-anthropic.png" alt="Anthropic" /></p>
  <p>Very good article <a
      href="https://www.anthropic.com/research/building-effective-agents">https://www.anthropic.com/research/building-effective-agents</a>
  </p>
  <h2 id="huggingface">HuggingFace</h2>
  <p><img src="./images/what-is-agent-huggingface.png" alt="HF" /></p>
  <h2 id="pydantic">Pydantic</h2>
  <p><img src="./images/what-is-agent-pydantic.png" alt="Pydantic" /></p>
  <p>We will look at examples of code to see what AI Agents are and what
    they can do.</p>
  <p>If we look at:</p>
  <h2 id="httpsaiagentsdirectorycomlandscape"><a
      href="https://aiagentsdirectory.com/landscape">https://aiagentsdirectory.com/landscape</a></h2>
  <p>We can see that there are many examples of AI Agent Frameworks and
    they seem to increase each week.</p>
  <p>At the beginning of the year it was in the 700s.</p>
  <h2 id="demystify-and-simplify">Demystify and simplify</h2>
  <p>What I would like to achieve in this talk is to
    <strong>demystify</strong> and <strong>simplify</strong> AI Agents and
    AI Programming because it can seem like it is another different world of
    dev.
  </p>
  <p>What if AI Agents were ‘just’ Python code with a REST API call,
    admittedly a very magical API?</p>
  <p><em>AI (Agents) as API</em>…</p>
  <p>Then, we would use day to day Python design patterns to handle the
    responses we get back from the AI Agent and move on to the next
    step.</p>
  <p>Business as usual for Python developers.</p>
  <p>This is the main focus of the talk - <strong>demystify and
      simplify</strong> - and this will enable you to create AI Agents and
    also construct workflows using AI Agents.</p>
  <p>With that in mind, we don’t need to fully grasp the python code this
    time around but focus on the ‘AI bit’ which I will highlight.</p>
  <p>It is more about seeing the high level view and one can dig deeper
    into the code offline.</p>
  <p>The repo is built as if a mini workshop with notebooks and
    <code>.py</code> files heavily commented.
  </p>
  <p><em>Look at the patterns and structure rather than the code
      details</em> - it is what helped me get to grips with this new
    paradigm.</p>
  <p>We will use Notebooks to explore AI Agents and then we will see an
    implementation of an AI Agent in a py file that combines many of the
    concepts we will discuss.</p>
  <h2 id="180-degrees">180 degrees</h2>
  <p><img src="./images/mouse-up.jpg" alt="mouse up" /> <img src="./images/mouse-down.jpg" alt="mouse down" /></p>
  <p>I like to use the metaphor of the upside down computer mouse. When we
    try to use it, it can take while to reverse our apporach. It is still
    the same set of movements - left, right, up and down - but in the
    opposite way to the way we are used to.</p>
  <p>There are 3 areas concerning Agentic AI in my opinion:</p>
  <ol type="1">
    <li>Client side creation of endpoints (APIs) rather than server side
      prebuilt endpoints.</li>
    <li>Use of Natural/Human Language, in my case English to create the
      code.</li>
    <li>Autonomy - the LLM directs the flow of the app.</li>
  </ol>
  <p>For the purpose of this talk I will use the term
    <code>function</code> in the mathematical sense:
  </p>
  <h3 id="input---functioninput---output---functionoutput---output2">input
    -&gt; function(input) -&gt; output -&gt; function(output) -&gt;
    output2</h3>
  <p>The function might be a variation on the Agent we are using or it may
    be another Agent that accepts the opuput as input. No different to
    Python Classes/Functions in an App.</p>
  <img src="./100_INPUT_OUTPUT.png" width=1000px>

  <p>The <code>function</code> might be a Python function or a class.</p>
  <p>Before we go into some code examples, we will refresh ourselves that
    a REST API a request is sending a payload of data to a server and then
    the server returns a response.</p>
  <p>This is a very simple example of a REST API.</p>
  <p>Again, this is to demystify and simplify any libraries we may import
    for convenience functions.</p>
  <p>Authentication takes place by passing some sort of token to the
    server, usually in the headers:</p>
  <pre><code>model = &quot;gpt-3.5-turbo&quot;
model_endpoint = &quot;https://api.openai.com/v1/chat/completions&quot;

## There is no reference to previous requests to maintain a history - we must supply the history with each request.

headers = {
    &quot;Content-Type&quot;: &quot;application/json&quot;,
    &quot;Authorization&quot;: f&quot;Bearer {api_key}&quot;,
}

# payload may vary from LLM Organisation but it is a text string.
payload = {
   &quot;model&quot;: model,
   &quot;messages&quot;: [
       {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: system_prompt},
       {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: user_prompt},
   ],
   &quot;stream&quot;: False,
   &quot;temperature&quot;: temperature, 
}

Low Temperature:
The bag is full of mostly blue marbles, with a few red and green. Low temperature means you&#39;re very likely to pull a blue marble, but you might occasionally get a red or green.

High Temperature:
The bag is filled with a mix of colors, and all colors are equally likely. High temperature means you&#39;re equally likely to pull any color, including the less common ones.
# Use HTTP POST method
response = requests.post(
   url=model_endpoinLow Temperature:
The bag is full of mostly blue marbles, with a few red and green. Low temperature means you&#39;re very likely to pull a blue marble, but you might occasionally get a red or green.
High Temperature:
The bag is filled with a mix of colors, and all colors are equally likely. High temperature means you&#39;re equally likely to pull any color, including the less common ones.t, # The API
   headers=headers, # Headers for authentication etc
   data=json.dumps(payload) # The request data we are sending
).json()

# KEY TAKEAWAYs

## There is only one endpoint. We don&#39;t use other endpoints for differing tasks, there is just one end point and we will create our custom endpoint through prompt engineering.
</code></pre>
  <p>The request is text and does not contain any objects or other data
    types.</p>
  <p>Likewise, we get a text response. We pass some text to a function and
    get some text back.</p>
  <p>We will look at <code>01_openai_api_with_requests.ipynb</code> to see
    an example of getting a response from the LLM.</p>
  <p><code>01_openai_api_with_requests.ipynb</code>.</p>
  <p>In <code>02_api.ipynb</code>, we can see that we can get a joke from
    a regular API endpoint, with the assumption that there is no AI
    involved!</p>
  <p>We can also ask OpenAI to tell us a joke…</p>
  <p>What if we want a more complex API endpoint/route?</p>
  <p>Let’s say we want to get a joke, get a rating as well as a verdict on
    whether it is worthy of publishing or whether a HUMAN should make the
    joke for publishing.</p>
  <p>We can do this by using a prompt.</p>
  <p>The prompt is the input to the AI agent.</p>
  <p>The prompt can be considered to be the API route we are creating and
    it it will be in Natural Language.</p>
  <p>Let’s look at this file <code>02_api.ipynb</code>…</p>
  <p>We can see this in <code>02_api.ipynb</code> where we pass a system
    prompt and then a prompt to create this endpoint, specifying how we want
    the data returned.</p>
  <p>This is effectively a new route for the API, but instead of it being
    coded on the server side by someone, it is coded on the client side,
    sent with the payload AND the code is NATURAL LANGUAGE.</p>
  <p>In the early days of ChatGPT, <em>prompt engineering</em> was often
    shownd as hacks or tricks. Nowdays, it seems far more structured and
    different LLMs use different schemas.</p>
  <p>(Current models are ‘Imperative’ in that we say what they are, what
    they do, how they do it…New REASONING models are ‘Declarative’ in that
    we say what we want them to do - the goal - what we want as output and
    what we might not want. The model then ‘reasons’ its way through the
    task.)</p>
  <p>We can think of it as pseudo-code which we may write whilst
    developing an app.</p>
  <p>In fact, it is like a person starting a new job. They will get a
    handbook of what the job involves, how to do it etc. and this is what we
    are doing with the LLM. <code>sample_prompt.md</code> is an example of
    this and how we could import content from a Markdown file.</p>
  <p>We set the system prompt to guide the AI agent, and then the prompt
    to create the endpoint.</p>
  <p>We can have more information than necessary and this can do no harm
    provide it is consistent and logical with the remaining prompt.
    Obviously, there will be more token usage but with the price going down,
    it is not an issue.</p>
  <p>We have covered 2/3 of the AI reverse process - Client Side creation
    of the route and the use of Natural Language.</p>
  <p>What about Autonomy?</p>
  <p>In our output, we asked the LLM to give not just a rating but a
    verdict on whether it is worthy of publishing or not. This is the
    <code>next</code> parameter that is returned. This is our own creation
    and we can have any key name.
  </p>
  <p>There are many software design patterns but essentially the next step
    in the app has been selected by the LLM. It is the <code>if/else</code>
    statement. or router.</p>
  <p>In summary, this module has shown the 3 counter-intuitive steps of AI
    Agents - Autonomy, Client Side Creation of the route and the use of
    Natural Language.</p>
  <p><img src="./101_ESSENCE.png" alt="ESSENCE" /></p>
  <h1 id="4-main-patterns">4 main patterns</h1>
  <p>Andrew Ng describes four main patterns</p>
  <p><a
      href="https://www.deeplearning.ai/the-batch/how-agents-can-improve-llm-performance/">https://www.deeplearning.ai/the-batch/how-agents-can-improve-llm-performance/</a>
  </p>
  <p><img src="./images/4-patterns.png" alt="patterns" /></p>
  <p>We have seen examples of these in this talk, bar a multi-agent
    pattern.</p>
  <h2 id="reflection-pattern">REFLECTION PATTERN</h2>
  <p><img src="./100_INPUT_OUTPUT.png" alt="REFLECTION" /></p>
  <p>input -&gt; function(input) -&gt; output -&gt; function(output) -&gt;
    output2</p>
  <p>We generate a response with our first query using a system prompt to
    create code.</p>
  <p>We then pass the output into another function that acts as a reviewer
    to produce the next version of the code.</p>
  <h2 id="tools">TOOLS</h2>
  <p>We have seen Tool Calling previously.</p>
  <h2 id="planning">PLANNING</h2>
  <p>We have seen planning previously.</p>
  <h1 id="when-might-we-use-llmsagents">When might we use
    LLMs/Agents?</h1>
  <p><img src="./102_WHEN.png" alt="open-ai" /></p>
  <h2 id="multi-agent">MULTI AGENT</h2>
  <h3 id="libraries">Libraries</h3>
  <p>I like to think of Libraries as frameworks without the framework! By
    this I mean we get building blocks to help us build things without
    having to conform to a building plan.</p>
  <h4 id="pydantic-ai">Pydantic AI</h4>
  <p>Pydantic is well known in everyday Python and is used by most AI
    Agent frameworks as structured data validation is vital.</p>
  <p>PydanticAI is a library/framework that uses Pydantic to create AI
    Agents.</p>
  <h4 id="huggingface-smolagents">Huggingface SmolAgents</h4>
  <p>HF SmolAgents is a library/framework that uses Huggingface
    Transformers to create AI Agents. It has broken new ground throught the
    use of its CodeAgent where tool calling is done via Python rather than
    JSON…show images…</p>
  <h3 id="crewsswarms">Crews/Swarms</h3>
  <p>Crews and Swarms are design patterns for MultiAgent collaboration.
    They each have their own use cases and we saw earlier that AI Agents can
    emit the ‘next’ step in the app which a range of desing patterns can
    harness.</p>
  <ul>
    <li><a
        href="https://aiagentsdirectory.com/category/ai-agents-frameworks">https://aiagentsdirectory.com/category/ai-agents-frameworks</a>
    </li>
  </ul>
  <h2 id="frameworks">Frameworks</h2>
  <p>There are many frameworks and libraries that can be used to create AI
    Agents. Some are more focused on the AI Agent and some are more focused
    on the UI.</p>
  <p>LlamaIndex Langchain Langraph AutoGen CrewAI</p>
  <p>And there are many low/no code versions as we saw in the <a
      href="https://aiagentsdirectory.com/category/ai-agents-frameworks">AI
      Agents Directory</a></p>
  <h1 id="testing-and-evaluation">Testing and Evaluation</h1>
  <p>We need a set of Ground Truths for various inputs for our workflow as
    well as a set of Ground Truths for tools and other functions used within
    an agent.</p>
  <p>Similar principles to regular testing but if the outputs are
    unstructured text then we need to find a way to verify accuracy. LLM as
    Judge or Human Evaluation can be used.</p>
  <p>The ROUTER example can be tested to see if the right report is
    selected for a set of Ground Truths.</p>
  <p>The FAQ example can be tested to see if answers to questions are
    correct.</p>
  <p><img src="./103_TESTS.png" alt="testing" /></p>
  <h2 id="httpsai-powered-knowledge-systemsnetlifyappevaluationoverview"><a
      href="https://ai-powered-knowledge-systems.netlify.app/evaluation/overview/">https://ai-powered-knowledge-systems.netlify.app/evaluation/overview/</a>
  </h2>
  <p>Useful article: <a
      href="https://medium.com/towards-generative-ai/judgeit-automating-rag-evaluation-using-llm-as-a-judge-d7c10b3f2eeb">LLM
      as Judge</a></p>
  <h2 id="summary">Summary</h2>
  <p>I hope AI Agents have been demystified and helped us understand what
    they can do, enabling us to either build our own frameworks or use
    existing ones, with a deeper appreciation and understanding of how they
    work.</p>
  <p><img src="./images/when-to-use-anthropic.png" alt="when to use" /></p>
  <p>Many talks on AI Agents stress using simple agents and introduce them
    bit by bit rather than create one large Agentic App.</p>
</body>

</html>