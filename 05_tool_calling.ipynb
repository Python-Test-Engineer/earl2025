{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30e2d04d",
   "metadata": {},
   "source": [
    "We have seen that the LLM needs a complete picture of what it needs to do.\n",
    "\n",
    "We have been adding extra data to the prompt and then running a new query.\n",
    "\n",
    "Sometimes we need extra information that we can't provide at the start of the prompt. We can gived a list of tools to the LLM explaining what each does.\n",
    "\n",
    "It can then select the right tool and arguments to get the extra data it needs. It will then send back the funtion and arguments that WE need to run, get the output and pass that into the next LLM request, building up a complete context picture for the LLM to answer.\n",
    "\n",
    "We saw in ROUTER how we can get the LLM to select the most appropriate report or in this case tool.\n",
    "\n",
    "When we say run it, the LLM passes 'us' information on the function that needs to be run and the arguments to be used. \n",
    "\n",
    "The LLM can also extract data from the prompt. We will see thuis more fullyu in `06_extraction_teachability.ipynb`\n",
    "\n",
    "We then run the function and send back the output to the LLM by appending to the list of messages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9410800f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d541dd13",
   "metadata": {},
   "source": [
    "As we are seeing, the LLM request is stateless and can only be as effective as the instructions and data we provide it.\n",
    "\n",
    "We can do this in a 'one time' request but if we don't know what to add in to the context, we can offer a set of tools and what they can offer to the LLM. \n",
    "\n",
    "It will then decide what it needs and let us know the functions and arguments WE need to run to then pass the output to the LLM to augment its context data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8dfced",
   "metadata": {},
   "source": [
    "In the cases where where an Agent calls a tool, the tool is called on our box.\n",
    "\n",
    "Where are the functions run? From the OpenAI website, we can see at the arrow that tools are executed on our 'box'.\n",
    "\n",
    "<img style=\"width:1100px;\" src=\"./images/where-tools-are-executed.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84142088",
   "metadata": {},
   "source": [
    "Like routing, we can also ask the LLM to select the most appropriate tool to use.\n",
    "\n",
    "In this example, we see how we can do Tool Selection and Data Extraction. The Agent determines what it will do next and also extracts data from the LLM response as arguments for the next step. \n",
    "\n",
    "In essence, the LLM decides on the tool/function it wants to use and send back the function name and arguments for us to run on our own box,\n",
    "get the result and then add this to the messages in the next LLM request.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83d4c84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm_client(llm_choice):\n",
    "    if llm_choice == \"GROQ\":\n",
    "        client = OpenAI(\n",
    "            base_url=\"https://api.groq.com/openai/v1\",\n",
    "            api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    "        )\n",
    "        return client\n",
    "    elif llm_choice == \"OPENAI\":\n",
    "        load_dotenv()  # load environment variables from .env fil\n",
    "        client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "        return client\n",
    "    else:\n",
    "        raise ValueError(\"Invalid LLM choice. Please choose 'GROQ' or 'OPENAI'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1179b4c5-cd1f-4131-a876-4c9f3f38d2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY exists and begins sk-proj-1WUVgv...\n",
      "GROQ_API_KEY exists and begins gsk_11hFN1EMfj...\n",
      "LLM_CHOICE: GROQ - MODEL: llama-3.3-70b-versatile\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "LLM_CHOICE = \"OPENAI\"\n",
    "LLM_CHOICE = \"GROQ\"\n",
    "\n",
    "if OPENAI_API_KEY:\n",
    "    print(f\"OPENAI_API_KEY exists and begins {OPENAI_API_KEY[:14]}...\")\n",
    "else:\n",
    "    print(\"OPENAI_API_KEY not set\")\n",
    "\n",
    "if GROQ_API_KEY:\n",
    "    print(f\"GROQ_API_KEY exists and begins {GROQ_API_KEY[:14]}...\")\n",
    "else:\n",
    "    print(\"GROQ_API_KEY not set\")\n",
    "\n",
    "\n",
    "client = get_llm_client(LLM_CHOICE)\n",
    "if LLM_CHOICE == \"GROQ\":\n",
    "    MODEL = \"llama-3.3-70b-versatile\"\n",
    "else:\n",
    "    MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "print(f\"LLM_CHOICE: {LLM_CHOICE} - MODEL: {MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd922b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original propmt\n",
    "system_message = \"\"\"\n",
    "You are an assistant that is very good at determining what tool to use to solve queries.\n",
    "\"\"\"\n",
    "\n",
    "ai_programming = \"\"\"\n",
    "\n",
    "# TOOLS\n",
    "\n",
    "You have two tools:\n",
    "\n",
    "1. A calculator tool that can perform basic arithmetic operations, such as addition, subtraction, multiplication, and division. If this tool is used then respond in JSON FORMAT.\n",
    "\n",
    "## Example JSON format:\n",
    "\n",
    "{\"tool\": \"calculator\", \"next\": \"do_calculation\", \"arguments\": {\"num1\": 5, \"num2\": 5, \"operation\": \"addition\"}} \n",
    "\n",
    "2. A jokes tool that can provide a light-hearted joke for the audience reqested. If this tool is used then respond in JSON FORMAT.\n",
    "\n",
    "## Example JSON format:\n",
    "\n",
    "{\"tool\": \"joke\", \"next\": \"do_joke\", \"audience\": \"Pythonistas\" }} \n",
    "\n",
    "\n",
    "NOTE: Only used tools if needed. If no tools are needed then respond in the usual way.\n",
    "\n",
    "Thank you for your being accurate and complete with this query.\n",
    "\"\"\"\n",
    "system_message += ai_programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fa6d2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"What is 120 plus 300?\"\n",
    "# user_prompt = \"Tell me a joke when I am doing stand up at a Python Conference\"\n",
    "# user_prompt = \"What is the capital of Ireland?\"\n",
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc2d8cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"tool\": \"calculator\", \"next\": \"do_calculation\", \"arguments\": {\"num1\": 120, \"num2\": 300, \"operation\": \"addition\"}}\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(model=MODEL, messages=prompts)\n",
    "print(response.choices[0].message.content)\n",
    "output = response.choices[0].message.content.replace(\"\\n\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46d61af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"tool\": \"calculator\", \"next\": \"do_calculation\", \"arguments\": {\"num1\": 120, \"num2\": 300, \"operation\": \"addition\"}}\n"
     ]
    }
   ],
   "source": [
    "# As specified in the AI Programming section of the prompt, we get back a JSON response/dictionary with what\n",
    "# the next step should be.\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c93881b",
   "metadata": {},
   "source": [
    "now at END OF AI BIT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d890bd",
   "metadata": {},
   "source": [
    "Having got the function and arguments, we extract these from the response and run it.\n",
    "\n",
    "This is now regular Python rather than AI. We have called the 'AI as API' and now handle the response in the same way we might handle the response from a DB query.\n",
    "\n",
    "I stress this as it is one of the personal outcomes - that Agentic Python is still traditional Python where our skills and experience are still needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fe3437e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tool': 'calculator', 'next': 'do_calculation', 'arguments': {'num1': 120, 'num2': 300, 'operation': 'addition'}}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    json_output = output.strip()\n",
    "\n",
    "    json_output = json.loads(json_output)\n",
    "\n",
    "    print(json_output)\n",
    "\n",
    "    # use do_next as 'next' is a reserved word\n",
    "\n",
    "    do_next = json_output[\"next\"]\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"No tool called so excepting...to be handled...\")\n",
    "    do_next = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55a24fe",
   "metadata": {},
   "source": [
    "Now we use our day to day Python to run some code based on the next step, _very much like calling a Weather API and then writing code based on the result._\n",
    "\n",
    "There are other ways we might code this but this will suffice for demonstration purposes.\n",
    "\n",
    "In this demo, we use the calculator tool or joke tool to give a response. The tool is some code rather than a defined functuion, we will see this in the PLANNING demo `08` and `09`, where we will use the tools to call functions in our code which we then feedback to the LLM to carry out the next stage of the plan.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34cfd6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this could be encapsulated into a function\n",
    "if do_next == \"do_joke\":\n",
    "    get_random_joke_internet = requests.get(\n",
    "        \"https://official-joke-api.appspot.com/random_joke\"\n",
    "    )\n",
    "    print(get_random_joke_internet.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "473647c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120 addition 300 = 420\n"
     ]
    }
   ],
   "source": [
    "# this could be encapsulated into a function\n",
    "# the main point is we have extracted the tool and data from the response\n",
    "if do_next == \"do_calculation\":\n",
    "    tool = json_output[\"tool\"]\n",
    "    num1 = json_output[\"arguments\"][\"num1\"]\n",
    "    num2 = json_output[\"arguments\"][\"num2\"]\n",
    "    operation = json_output[\"arguments\"][\"operation\"]\n",
    "    if operation == \"addition\":\n",
    "        answer = num1 + num2\n",
    "    elif operation == \"subtraction\":\n",
    "        answer = num1 - num2\n",
    "    elif operation == \"multiplication\":\n",
    "        answer = num1 * num2\n",
    "    elif operation == \"division\":\n",
    "        answer = num1 / num2\n",
    "    print(f\"{num1} {operation} {num2} = {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16412c72",
   "metadata": {},
   "source": [
    "<img src=\"./images/05-tool.png\" width=700px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5003e17b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
