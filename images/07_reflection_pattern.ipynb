{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "514b9aab",
   "metadata": {},
   "source": [
    "<img src=\"./101_ESSENCE.png\" width=700px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f4d7b7-40bf-43b9-a626-2a11d5529ac8",
   "metadata": {},
   "source": [
    "### Reflection pattern\n",
    "\n",
    "## input -> function(input) -> output -> function(output) -> output2\n",
    "\n",
    "<img src=\"./100_INPUT_OUTPUT.png\" width=700px>\n",
    "\n",
    "Each LLM call is stateless so the LLM sees it for the first time. The more information (context) it has the better will be its response.\n",
    "\n",
    "We can use this pattern to implement a reflection pattern in stages or we could have gathered the information in another way and passed it in a single request as the first time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874e847f",
   "metadata": {},
   "source": [
    "## Agent One is a code expert. \n",
    "\n",
    "## Agent Two is an expert code reviewer.\n",
    "\n",
    "## Agent Three then combines the two to create a final output.\n",
    "\n",
    "Files `200_code.md`, `200_critique.md` and `200_final.md` are produced rather than outputs here.\n",
    "\n",
    "We generate a response with our first query using a system prompt to create code.\n",
    "\n",
    "We then pass the output into another function that acts as a reviewer to produce the next version of the code.\n",
    "\n",
    "This can be repeated until we reach certain criteria or MAX_ITERATIONS, whichever comes first.\n",
    "\n",
    "Each query is as if it was the first query with more information contained within it as each request is stateless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96731d2f-a079-4e41-9756-220f02d4ebd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display_markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9737534a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm_client(llm_choice):\n",
    "    if llm_choice == \"GROQ\":\n",
    "        client = OpenAI(\n",
    "            base_url=\"https://api.groq.com/openai/v1\",\n",
    "            api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    "        )\n",
    "        return client\n",
    "    elif llm_choice == \"OPENAI\":\n",
    "        load_dotenv()  # load environment variables from .env fil\n",
    "        client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "        return client\n",
    "    else:\n",
    "        raise ValueError(\"Invalid LLM choice. Please choose 'GROQ' or 'OPENAI'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2af18ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY exists and begins sk-proj-1WUVgv...\n",
      "GROQ_API_KEY exists and begins gsk_11hFN1EMfj...\n",
      "LLM_CHOICE: GROQ - MODEL: llama-3.3-70b-versatile\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "LLM_CHOICE = \"OPENAI\"\n",
    "LLM_CHOICE = \"GROQ\"\n",
    "\n",
    "if OPENAI_API_KEY:\n",
    "    print(f\"OPENAI_API_KEY exists and begins {OPENAI_API_KEY[:14]}...\")\n",
    "else:\n",
    "    print(\"OPENAI_API_KEY not set\")\n",
    "\n",
    "if GROQ_API_KEY:\n",
    "    print(f\"GROQ_API_KEY exists and begins {GROQ_API_KEY[:14]}...\")\n",
    "else:\n",
    "    print(\"GROQ_API_KEY not set\")\n",
    "\n",
    "\n",
    "client = get_llm_client(LLM_CHOICE)\n",
    "if LLM_CHOICE == \"GROQ\":\n",
    "    MODEL = \"llama-3.3-70b-versatile\"\n",
    "else:\n",
    "    MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "print(f\"LLM_CHOICE: {LLM_CHOICE} - MODEL: {MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e644a635-e035-44e2-8c25-cee0f2b56556",
   "metadata": {},
   "source": [
    "We will start the **\"generation\"** chat history with the system prompt, as we said before. In this case, let the LLM act like a Python\n",
    "programmer eager to receive feedback / critique by the user.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12467256-c741-495a-9923-439c1fcf270d",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = \"\"\"\n",
    "You are a Python programmer tasked with generating high quality Python code. Your task is to Generate the best content possible for the user's request. If the user requests critique, respond with a revised version of your previous attempt.\n",
    "\"\"\"\n",
    "\n",
    "generation_chat_history = [{\"role\": \"system\", \"content\": content}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43149b4f-54db-455f-9d39-6ad2f5c52b94",
   "metadata": {},
   "source": [
    "Now, as the user, we are going to ask the LLM to generate an implementation of the Requests library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0742e7bd-4857-4ed1-a96b-37098d448bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_chat_history.append(\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Generate a Python implementation of requesting an API with the Requests library\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df1bffe-375f-4a9a-8433-e217eb94aea2",
   "metadata": {},
   "source": [
    "Let's generate the first version of the essay.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff984277-733c-4495-b7fd-0669393380b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_code = (\n",
    "    client.chat.completions.create(messages=generation_chat_history, model=MODEL)\n",
    "    .choices[0]\n",
    "    .message.content\n",
    ")\n",
    "\n",
    "\n",
    "generation_chat_history.append({\"role\": \"assistant\", \"content\": user_code})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c03f208b-2234-4fd1-a02b-f4fff06c01a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_markdown(user_code, raw=True)\n",
    "# Output below...\n",
    "with open(\"200_code.md\", \"w\") as f:\n",
    "    f.write(user_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a04ebe5-0573-4520-a529-aff22d486b7d",
   "metadata": {},
   "source": [
    "## Reflection Step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020433f1",
   "metadata": {},
   "source": [
    "This is equivalent to asking a follow up question in say ChatGPT and we change the system prompt or what we want it to do along with the output from the previous query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d93c928-d585-48af-a74c-a5b8d84593c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reflection_chat_history = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are an experienced and talented Pythonista. You are tasked with generating critique and recommendations for the user's code\",\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26af1a73-4d91-40e8-a9bc-c34d32b2ab82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We add new messages to the list of messages so that the LLM has context and knowledge of what proceeded.\n",
    "\n",
    "# LLM calls are stateless and previous messages are not stored with the LLM. This is an important fact as we do not want to go over the context window for the LLM or incur unwanted costs if applicable.\n",
    "\n",
    "reflection_chat_history.append({\"role\": \"user\", \"content\": user_code})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa994c8-3612-47b0-9571-e21d0d73d896",
   "metadata": {},
   "source": [
    "CRITIQUE TIME\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a628b75c",
   "metadata": {},
   "source": [
    "Now that we have the context and the request for a critique, we make a request to the LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40fee42f-d47a-41b1-a40d-7208ba76ce98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "critique = (\n",
    "    client.chat.completions.create(messages=reflection_chat_history, model=MODEL)\n",
    "    .choices[0]\n",
    "    .message.content\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fef3203-c7f1-407f-8b9b-4e8ae140a4cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display\n",
    "\n",
    "# display_markdown(critique, raw=True)\n",
    "# Critique displayed below...\n",
    "with open(\"200_critique.md\", \"w\") as f:\n",
    "    f.write(user_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df433b0-d662-4378-895e-6b09dd3201bc",
   "metadata": {},
   "source": [
    "Add CRITIQUE to chat...\n",
    "\n",
    "Notice how we are appending previous responses so that the next SYSTEM MESSAGE has history or context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27a85bb3-cf6a-4576-8caf-cd41e602a1f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generation_chat_history.append({\"role\": \"user\", \"content\": critique})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c1aefa-8454-41ab-af40-2675f340a577",
   "metadata": {},
   "source": [
    "Response to CRITIQUE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91d845cf-51c3-4cfd-b6a7-1b970413f6db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "essay = (\n",
    "    client.chat.completions.create(messages=generation_chat_history, model=MODEL)\n",
    "    .choices[0]\n",
    "    .message.content\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef14eaa8-f501-4efc-997f-8564ec8dccd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Diaply user response to CRITIQUE\n",
    "# display_markdown(essay, raw=True)\n",
    "# Response to critique displayed below...\n",
    "with open(\"200_final.md\", \"w\") as f:\n",
    "    f.write(user_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75883af2-f31d-4c24-b1ff-315a0711f9fa",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### We can of course make this a Class...\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
